---
title: "Exercises with Solutions"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    theme: paper
    code_folding: show
    math_method: katex
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {rmarkdown::render(input, output_dir = "../docs")})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE
)
```

# Chapter 1: Basics and Linear Models

## Exercise on linear regression

```{r}
library(tidyverse)

diamonds <- mutate_if(diamonds, is.ordered, factor, ordered = FALSE)

fit <- lm(price ~ carat + color + cut + clarity, data = diamonds)
summary(fit)
```

**Comments**

- **Model quality:** About 92% of price variations are explained by covariates. Typical prediction error is 1157 USD.
- **Effects:** All effects point into the intuitively right direction (larger stones are more expensive, worse color are less expensive etc.)
- **Practical perspective:** Additivity in color, cut and clarity are not making sense. Their effects should get larger with larger diamond size. This can be solved by adding interaction terms with carat or, much easier, to switch to a logarithmic response.
 
## Exercise on GLMs

```{r}
library(tidyverse)

diamonds <- mutate_if(diamonds, is.ordered, factor, ordered = FALSE)

fit <- glm(
  price ~ log(carat) + color + cut + clarity, 
  data = diamonds, 
  family = Gamma(link = "log")
)
summary(fit)

mean(predict(fit, type = "response")) / mean(diamonds$price) - 1
```

**Comment:** The coefficients are very similar to the linear regression with log(price) as response. This makes sense as we interpret the coefficients in the same way! The bias is small, but not exactly 0 (because log is not the natural link of the Gamma GLM).

# Chapter 2: Model Selection and Validation

## Exercise 1

```{r}
library(ggplot2)
library(FNN)
library(withr)

RMSE <- function(y, pred) {
  sqrt(mean((y - pred)^2))
}

y <- "price"  
x <- c("carat", "color", "cut", "clarity")

dia <- diamonds[, c(y, x)]
dia <- unique(dia)  #  -> only new row

# Split diamonds into 80% for training and 20% for testing
with_seed(
  9838,
  ix <- sample(nrow(dia), 0.8 * nrow(dia))
)

train <- dia[ix, ]
test <- dia[-ix, ]

y_train <- train[[y]]
y_test <- test[[y]]

# Standardize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to test data
X_test <- scale(
  data.matrix(test[, x]),
  center = attr(X_train, "scaled:center"),
  scale = attr(X_train, "scaled:scale")
)

# Split training data into folds
nfolds <- 5
with_seed(
  9838,
  fold_ix <- sample(1:nfolds, nrow(train), replace = TRUE)
)

# Cross-validation performance of k-nearest-neighbor for k = 1-20
paramGrid <- data.frame(RMSE = NA, k = 1:20)

for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  scores <- numeric(nfolds)
  
  for (fold in 1:nfolds) {
    X_train_cv <- X_train[fold_ix != fold, ]
    y_train_cv <- y_train[fold_ix != fold]
    
    X_valid_cv <- X_train[fold_ix == fold, ]
    y_valid_cv <- y_train[fold_ix == fold]
    
    pred <- knn.reg(X_train_cv, test = X_valid_cv, k = k, y = y_train_cv)$pred
    scores[fold] <- RMSE(y_valid_cv, pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}

# Best CV performance
head(paramGrid[order(paramGrid$RMSE), ], 2)

# Cross-validation performance of linear regression
rmse_reg <- numeric(nfolds)

for (fold in 1:nfolds) {
  fit <- lm(reformulate(x, y), data = train[fold_ix != fold, ])
  pred <- predict(fit, newdata = train[fold_ix == fold, ])
  rmse_reg[fold] <- RMSE(y_train[fold_ix == fold], pred)
}
(rmse_reg <- mean(rmse_reg))

# The overall best model is 6-nearest-neighbor
pred <- knn.reg(X_train, test = X_test, k = 6, y = y_train)$pred

# Test performance for the best model
RMSE(y_test, pred)
```

**Comments:** The test performance of the best model (6-NN) seems clearly worse than the one without deduplication (~700 USD RMSE vs ~600). CV performance well corresponds to test performance. Overall, this is probably the more realistic performance than the one obtained from the original data set. Still, as certain rows could be identical by chance, our deduplication approach might be slightly too conservative. The true performance will probably be somewhere between the two approaches.

## Exercise 2

```{r}
library(ggplot2)
library(withr)
library(MetricsWeighted)

# Split diamonds into 90% for training and 10% for testing
with_seed(
  seed = 343,   
  ix <- sample(nrow(diamonds), 0.9 * nrow(diamonds))
)

train <- diamonds[ix, ]
test <- diamonds[-ix, ]

# manual GridSearchCV
nfolds <- 5
with_seed(
  seed = 9387,  
  fold_ix <- sample(1:nfolds, nrow(train), replace = TRUE)
)
paramGrid <- data.frame(Deviance = NA, k = 1:12)

for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  scores <- numeric(nfolds)
  
  for (fold in 1:nfolds) {
    fit <- glm(
      price ~ poly(log(carat), degree = k) + color + cut + clarity, 
      data = train[fold_ix != fold, ], 
      family = Gamma(link = "log")
    )
    pred <- predict(fit, train[fold_ix == fold, ], type = "response")
    scores[fold] <- deviance_gamma(train$price[fold_ix == fold], pred)
  }
  paramGrid[i, "Deviance"] <- mean(scores)
}

paramGrid
paramGrid[order(paramGrid$Deviance), ]

# Fit best model on full training data
fit <- glm(
  price ~ poly(log(carat), degree = 8) + color + cut + clarity, 
  data = train, 
  family = Gamma(link = "log")
)

# Evaluate on test data
pred <- predict(fit, test, type = "response")
deviance_gamma(test$price, pred) # 0.01697816
```

**Comments:** The optimal degree seems to be 8 with a CV mean deviance of 0.01576.
There seems to be some CV overfit as the mean deviance of the test data is substantially larger. Caution: Instead of using such high degree polynomial, it is almost always better to use regression splines. What would you get then?

## Exercise 3 (optional)

Solution not shown here.

# Chapter 3: Trees

## Exercises on Random Forests

### Exercise 1

```{r}
library(tidyverse)
library(splitTools)
library(ranger)
library(MetricsWeighted)

dia <- diamonds %>% 
  mutate(log_carat = log(carat))

# Train/test split
ix <- partition(dia$price, p = c(train = 0.8, test = 0.2), seed = 9838)

fit <- ranger(
  price ~ log_carat + color + cut + clarity, 
  num.trees = 500,
  data = dia[ix$train, ], 
  importance = "impurity",
  seed = 83
)
fit

# Performance on test data
pred <- predict(fit, dia[ix$test, ])$predictions
rmse(dia$price[ix$test], pred)
```

**Comment:** The results are essentially identical because log is a monotonic transformation. Differences might come from implementation tricks.

### Exercise 2

```{r}
library(tidyverse)
library(splitTools)
library(ranger)
library(flashlight)
library(MetricsWeighted)
library(insuranceData)

data(dataCar)

# Train/test split (stratified on response)
ix <- partition(dataCar$clm, p = c(train = 0.8, test = 0.2), seed = 9838)

# Instead of systematic grid search, manually select good tree depth by OOB
fit <- ranger(
  clm ~ veh_value + veh_body + veh_age + gender + area + agecat,
  data = dataCar[ix$train, ], 
  probability = TRUE, 
  max.depth = 5,
  importance = "impurity"
)
fit # OOB prediction using Brier score (= MSE) 0.06340884 

# Note: Brier score is the same as the MSE, applied to binary data. It is
# not a bad evaluation criterion in such situation, 
# so we will use this within this example.
pred <- predict(fit, dataCar[ix$test, ])$predictions[, 2]
mse(dataCar[ix$test, "clm"], pred)  # 0.06337011
r_squared(dataCar[ix$test, "clm"], pred) # 0.002069925

# Alternative to Brier score: relative log loss/deviance improvement
r_squared_bernoulli(dataCar[ix$test, "clm"], pred) # 0.004246408
```

**Comment:** Test performance with small tree depth seems to be best. When studying relative performance metrics like the relative deviance gain, we can see that performance of the model is very low. TPL claims seem to be mostly determined by bad luck, which makes sense.

```{r}
# Variable importance regarding Gini improvement
imp <- sort(importance(fit))
imp <- imp / sum(imp)
barplot(imp, horiz = TRUE, col = "chartreuse4", cex.names = 0.8, las = 2)

# Partial dependence plot for the strongest predictor "veh_value"
fl <- flashlight(
  model = fit,
  data = dataCar[ix$train, ], 
  label = "rf", 
  predict_function = function(m, X) predict(m, X)$predictions[, 2]
)

for (v in all.vars(~veh_value + veh_body + veh_age + gender + area + agecat)) {
  p <- light_profile(fl, v = v, n_bins = 40) %>% 
    plot(color = "chartreuse4", rotate_x = (v == "veh_body")) +
    labs(title = paste("PDP for", v), y = "Prediction") 
  print(p)
}
```

## Exercises on Boosting

### Exercise 1

Just copy paste parts of the code in the lecture notes.

```{r}
library(tidyverse)
library(xgboost)
library(splitTools)

y <- "price"
x <- c("carat", "color", "cut", "clarity")

# Split into train and test
ix <- partition(diamonds[[y]], p = c(train = 0.8, test = 0.2), seed = 9838)

y_train <- diamonds[[y]][ix$train]
X_train <- data.matrix(diamonds[ix$train, x])

# XGBoost data interface
dtrain <- xgb.DMatrix(X_train, label = y_train)

# Load grid and select best iteration. If interactive, use r/gridsearch/...
grid <- readRDS("gridsearch/diamonds_xgb.rds")
grid <- grid[order(grid$cv_score), ]

# Monotone constraints for carat
params <- as.list(grid[1, -(1:3)])
params$monotone_constraints <- c(1, 0, 0, 0)

# Fit final, tuned model with monotone constraints for carat
fit <- xgb.train(
  params = params, 
  data = dtrain, 
  nrounds = grid[1, "iteration"]
)

# Partial dependence plot for carat
library(flashlight)

fl <- flashlight(
  model = fit,
  data = diamonds[ix$train, ], 
  label = "xgb", 
  predict_function = function(m, X) predict(m, data.matrix(X[x]))
)

light_profile(fl, v = "carat", n_bins = 40) %>% 
  plot(color = "chartreuse4") +
  labs(title = "PDP for carat", y = "Prediction") 
```

**Comment:** The argument is called "monotone_constraints". For each covariate, a value 0 means no constraint, a value -1 means a negative constraints, and a value 1 means positive constraint. Applying the constraint now leads to a monotonically increasing partial dependence plot. This is extremely useful in practice. Besides monotonic constraints, also interaction constraints are possible.

### Exercise 2

```{r}
# We simply adapt the template from the script

library(tidyverse)
library(xgboost)
library(splitTools)
library(insuranceData)

data(dataCar)

y <- "clm"
x <- c("veh_value", "veh_body", "veh_age", "gender", "area", "agecat")

# Split into train and test
ix <- partition(dataCar[[y]], p = c(train = 0.8, test = 0.2), seed = 9838)

y_train <- dataCar[[y]][ix$train]
X_train <- data.matrix(dataCar[ix$train, x])

y_test <- dataCar[[y]][ix$test]
X_test <- data.matrix(dataCar[ix$test, x])

# XGBoost data handler
dtrain <- xgb.DMatrix(X_train, label = y_train)

# If grid search is to be run again, set tune <- TRUE
tune <- FALSE

if (tune) {
  # Use default parameters to set learning rate with suitable number of rounds
  params <- list(
    learning_rate = 0.02,
    objective = "binary:logistic",
    eval_metric = "logloss"
  )
  
  # Cross-validation
  cvm <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 5000,
    nfold = 5,
    early_stopping_rounds = 20,
    showsd = FALSE, 
    print_every_n = 50
  )
  cvm 
  
  # Grid
  grid <- expand.grid(
    iteration = NA,
    cv_score = NA,
    train_score = NA,
    learning_rate = 0.02,
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = 3:6, 
    min_child_weight = c(0.1, 1),
    colsample_bytree = c(0.8, 1), 
    subsample = c(0.8, 1), 
    reg_lambda = c(0, 2.5, 5, 7.5),
    reg_alpha = c(0, 4),
    # tree_method = "hist",   # when data is large
    min_split_loss = c(0, 1e-04)
  )
  
  # Grid search or randomized search if grid is too large
  max_size <- 20
  grid_size <- nrow(grid)
  if (grid_size > max_size) {
    grid <- grid[sample(grid_size, max_size), ]
    grid_size <- max_size
  }
  
  # Loop over grid and fit XGBoost with five-fold CV and early stopping
  pb <- txtProgressBar(0, grid_size, style = 3)
  for (i in seq_len(grid_size)) {
    cvm <- xgb.cv(
      params = as.list(grid[i, -(1:2)]),
      data = dtrain,
      nrounds = 5000,
      nfold = 5,
      early_stopping_rounds = 20,
      verbose = 0
    )
    
    # Store result
    grid[i, 1] <- cvm$best_iteration
    grid[i, 2:3] <- cvm$evaluation_log[, c(4, 2)][cvm$best_iteration]
    setTxtProgressBar(pb, i)
    
    # Save grid to survive hard crashs. If interactive, use r/gridsearch/...
    saveRDS(grid, file = "gridsearch/claims_xgb.rds")
  }
}

# Load grid and select best iteration. If interactive use r/gridsearch/...
grid <- readRDS("gridsearch/claims_xgb.rds")
grid <- grid[order(grid$cv_score), ]
head(grid)

# Fit final, tuned model
fit <- xgb.train(
  params = as.list(grid[1, -(1:3)]), 
  data = dtrain, 
  nrounds = grid[1, "iteration"]
)

# Interpretation
library(MetricsWeighted)
library(flashlight)

# Performance on test data
pred <- predict(fit, X_test)
deviance_bernoulli(y_test, pred)

# Relative performance
r_squared_bernoulli(y_test, pred)
# Relative performance gain is very low, but very slightly better than 
# of the tuned random forest.

# Variable importance regarding total loss improvement
imp <- xgb.importance(model = fit)
xgb.plot.importance(imp)

# Partial dependence plots
fl <- flashlight(
  model = fit, 
  data = dataCar[ix$train, ], 
  label = "xgb", 
  predict_function = function(m, X) predict(m, data.matrix(X[x]))
)

for (v in x) {
  p <- light_profile(fl, v = v, n_bins = 40) %>% 
    plot(color = "chartreuse4", rotate_x = (v == "veh_body")) +
    labs(title = paste("PDP for", v), y = "Prediction") 
  print(p)
}
```

### Exercise 3 (Optional)

```{r}
# We slightly adapt the XGBoost template

library(tidyverse)
library(lightgbm)
library(splitTools)
library(insuranceData)

data(dataCar)

y <- "clm"
x <- c("veh_value", "veh_body", "veh_age", "gender", "area", "agecat")

# Split into train and test
ix <- partition(dataCar[[y]], p = c(train = 0.8, test = 0.2), seed = 9838)

y_train <- dataCar[[y]][ix$train]
X_train <- data.matrix(dataCar[ix$train, x])

y_test <- dataCar[[y]][ix$test]
X_test <- data.matrix(dataCar[ix$test, x])

# We could even set categorical_feature = "veh_body" in order
# to treat that feature unordered categorical
dtrain <- lgb.Dataset(
  X_train, label = y_train, params = list(feature_pre_filter = FALSE)
)

# If grid search is to be run again, set tune <- TRUE
tune <- FALSE

if (tune) {
  # Use default parameters to set learning rate with suitable number of rounds
  params <- list(
    learning_rate = 0.002,
    objective = "binary",
    metric = "binary_logloss",
    num_threads = 4  # choose number of physical cores
  )
  
  # Cross-validation
  cvm <- lgb.cv(
    params = params,
    data = dtrain,
    nrounds = 5000,
    nfold = 5,
    early_stopping_rounds = 20,
    eval_freq = 50
  )
  cvm 
  
  # Grid
  grid <- expand.grid(
    iteration = NA,
    score = NA,
    learning_rate = 0.002,
    objective = "binary",
    metric = "binary_logloss",
    num_leaves = c(15, 31, 63),
    min_data_in_leaf = c(10, 20, 50), # unclear whether this works without calling lgb.Dataset()
    min_sum_hessian_in_leaf = c(0.001, 0.1),
    colsample_bynode = c(0.8, 1), 
    bagging_fraction = c(0.8, 1), 
    lambda_l1 = c(0, 4),
    lambda_l2 = c(0, 2.5, 5, 7.5),
    num_threads = 4,  # choose number of physical cores
    stringsAsFactors = FALSE
  )
  
  # Grid search or randomized search if grid is too large
  max_size <- 20
  grid_size <- nrow(grid)
  if (grid_size > max_size) {
    grid <- grid[sample(grid_size, max_size), ]
    grid_size <- max_size
  }
  
  # Loop over grid and fit LGB with five-fold CV and early stopping
  pb <- txtProgressBar(0, grid_size, style = 3)
  for (i in seq_len(grid_size)) {
    cvm <- lgb.cv(
      params = as.list(grid[i, -(1:2)]),
      data = dtrain,
      nrounds = 5000,
      nfold = 5,
      early_stopping_rounds = 20,
      verbose = -1
    )
    
    # Store result
    grid[i, 1:2] <- as.list(cvm)[c("best_iter", "best_score")]
    setTxtProgressBar(pb, i)
    
    # Save grid to survive hard crashs. If interactive, use r/gridsearch/...
    saveRDS(grid, file = "gridsearch/claims_lgb.rds")
  }
}

# Load grid and select best iteration. If interactive use r/gridsearch/...
grid <- readRDS("gridsearch/claims_lgb.rds")
grid <- grid[order(grid$score), ]
head(grid)

# Fit final, tuned model
fit <- lgb.train(
  params = as.list(grid[1, -(1:2)]), 
  data = dtrain, 
  nrounds = grid[1, "iteration"]
)

# Interpretation
library(MetricsWeighted)
library(flashlight)

# Performance on test data
pred <- predict(fit, X_test)
deviance_bernoulli(y_test, pred)

# Relative performance
r_squared_bernoulli(y_test, pred) # 0.00439
# Relative performance gain is a bit lower than with XGB

# Variable importance regarding total loss improvement
imp <- lgb.importance(model = fit)
lgb.plot.importance(imp)

# Some partial dependence plots
fl <- flashlight(
  model = fit, 
  data = dataCar[ix$train, ], 
  label = "lgb", 
  predict_function = function(m, X) predict(m, data.matrix(X[, x]))
)

for (v in x) {
  p <- light_profile(fl, v = v, n_bins = 40) %>% 
    plot(color = "chartreuse4", rotate_x = (v == "veh_body")) +
    labs(title = paste("PDP for", v), y = "Prediction") 
  print(p)
}
```

# Chapter 4: Neural Nets

## Exercise 1

```{r}
library(tidyverse)
library(withr)
library(keras)

# Path to conda env with TensorFlow
keras::use_condaenv("C:/Users/Michael/anaconda3/envs/ml_lecture")

y <- "price"
x <- c("carat", "color", "cut", "clarity")

with_seed(
  9838, 
  ix <- sample(nrow(diamonds), 0.8 * nrow(diamonds))
)

train <- diamonds[ix, ]
test <- diamonds[-ix, ]

X_train <- train[, x]
X_test <- test[, x]

# Integers are not auto-cast by all TF versions...
y_train <- as.numeric(train[[y]])
y_test <- as.numeric(test[[y]])

# Standardize X using X_train
temp <- scale(data.matrix(X_train))
sc <- list(
  center = attr(temp, "scaled:center"), 
  scale = attr(temp, "scaled:scale")
)

# Function that maps data to scaled network input
prep_nn <- function(X, sel = x, scaling = sc) {
  X <- data.matrix(X[, sel, drop = FALSE])
  scale(X, center = scaling$center, scale = scaling$scale)
}

loss_gamma <- function(y_true, y_pred) {
  -k_log(y_true / y_pred) + y_true / y_pred
}

# Trying to make things reproducible...
k_clear_session()
tensorflow::set_random_seed(49)

# Input layer: we have 4 covariates
input <- layer_input(shape = 4)

# Two hidden layers with contracting number of nodes
output <- input %>%
  layer_dense(units = 30, activation = "tanh") %>% 
  layer_dense(units = 15, activation = "tanh") %>% 
  layer_dense(units = 1, activation = k_exp)

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
summary(nn)

nn %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.001),
  loss = loss_gamma
)

# Callbacks
cb <- list(
  callback_early_stopping(patience = 20),
  callback_reduce_lr_on_plateau(patience = 5)
)

# Fit model
history <- nn %>% fit(
  x = prep_nn(X_train),
  y = y_train,
  epochs = 200,
  batch_size = 400, 
  validation_split = 0.2,
  callbacks = cb
)

plot(history, metrics = "loss", smooth = FALSE) +
  coord_cartesian(ylim = c(0, 5))

# Interpret
library(flashlight)
library(MetricsWeighted)

fl <- flashlight(
  model = nn, 
  y = "price", 
  data = test, 
  label = "nn", 
  metrics = list(
    `Gamma deviance` = deviance_gamma, 
    `Relative deviance reduction` = r_squared_gamma
  ),
  predict_function = function(m, X) 
    predict(m, prep_nn(X), batch_size = 1000, verbose = 0)
)

# Performance on test data
light_performance(fl)$data

# Permutation importance
light_importance(fl, v = x) %>% 
  plot(fill = "orange")

# Partial dependence plots
for (v in x) {
  p <- light_profile(fl, v = v, n_bins = 40) %>% 
    plot(color = "chartreuse4") +
    ggtitle(paste("PDP for", v)) 
  print(p)
}
```

## Exercise 2

See lecture notes for a solution with embeddings.
